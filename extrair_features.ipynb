{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from skimage.feature import hog\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "\n",
    "from tensorflow.keras.applications import VGG16, VGG19\n",
    "\n",
    "from tensorflow.keras.applications import vgg16\n",
    "from tensorflow.keras.applications import vgg19\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defina os diretórios\n",
    "source_dir = 'selected_images_dog_cat'\n",
    "\n",
    "uploaded_images = []\n",
    "# Percorra os arquivos no diretório de origem\n",
    "for filename in os.listdir(source_dir):\n",
    "    source_file = os.path.join(source_dir, filename)\n",
    "    source_file = source_file.replace('\\\\', '/')\n",
    "    uploaded_images.append(source_file)\n",
    "\n",
    "print(uploaded_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hog(img_size, pixel_per_cell):\n",
    "    hog_features = []\n",
    "    for filename in uploaded_images:\n",
    "        image = imread(filename)\n",
    "        image_resized = resize(image, (img_size, img_size))\n",
    "        fd, hog_image = hog(image_resized, orientations=9, pixels_per_cell=(pixel_per_cell, pixel_per_cell),\n",
    "                            cells_per_block=(2, 2), visualize=True, channel_axis=-1)\n",
    "        hog_features.append(fd)\n",
    "    return hog_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hog_128_16 = extract_hog(128, 16)\n",
    "hog_128_20 = extract_hog(128, 20)\n",
    "hog_256_16 = extract_hog(256, 16)\n",
    "hog_256_20 = extract_hog(256, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cnn(img_size, model, vgg):\n",
    "    cnn_features = []\n",
    "    for filename in uploaded_images:\n",
    "        imagem = imread(filename)\n",
    "        ##print(imagem.shape)\n",
    "        print(filename)\n",
    "        imagem_reduzida = resize(imagem, (img_size, img_size))\n",
    "        x = image.img_to_array(imagem_reduzida)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = vgg.preprocess_input(x)\n",
    "        features = model.predict(x)\n",
    "        features_flatten_vgg = features.flatten()\n",
    "        cnn_features.append(features_flatten_vgg)\n",
    "    return cnn_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_16_avg = VGG16(weights='imagenet', include_top=False, pooling='avg')\n",
    "model_16_max = VGG16(weights='imagenet', include_top=False, pooling='max')\n",
    "model_19_avg = VGG19(weights='imagenet', include_top=False, pooling='avg')\n",
    "model_19_max = VGG19(weights='imagenet', include_top=False, pooling='max')\n",
    "\n",
    "cnn_16_avg_128 = extract_cnn(128, model_16_avg, vgg16)\n",
    "cnn_16_max_128 = extract_cnn(128, model_16_max, vgg16)\n",
    "\n",
    "cnn_16_avg_256 = extract_cnn(256, model_16_avg, vgg16)\n",
    "cnn_16_max_256 = extract_cnn(256, model_16_max, vgg16)\n",
    "\n",
    "cnn_19_avg_128 = extract_cnn(128, model_19_avg, vgg19)\n",
    "cnn_19_max_128 = extract_cnn(128, model_19_max, vgg19)\n",
    "\n",
    "cnn_19_avg_256 = extract_cnn(256, model_19_avg, vgg19)\n",
    "cnn_19_max_256 = extract_cnn(256, model_19_max, vgg19)\n",
    "\n",
    "# print(cnn_16_avg_128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dict = {\n",
    "    'hog_128_16': hog_128_16,\n",
    "    'hog_128_20': hog_128_20,\n",
    "    'hog_256_16': hog_256_16,\n",
    "    'hog_256_20': hog_256_20,\n",
    "    'cnn_16_avg_128': cnn_16_avg_128,\n",
    "    'cnn_16_max_128': cnn_16_max_128,\n",
    "    'cnn_16_avg_256': cnn_16_avg_256,\n",
    "    'cnn_16_max_256': cnn_16_max_256,\n",
    "    'cnn_19_avg_128': cnn_19_avg_128,\n",
    "    'cnn_19_max_128': cnn_19_max_128,\n",
    "    'cnn_19_avg_256': cnn_19_avg_256,\n",
    "    'cnn_19_max_256': cnn_19_max_256\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Function to convert numpy arrays to lists in the dictionary\n",
    "def convert_ndarrays(obj):\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: convert_ndarrays(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_ndarrays(i) for i in obj]\n",
    "    return obj\n",
    "\n",
    "# Convert all ndarrays to lists\n",
    "serializable_features_dict = convert_ndarrays(features_dict)\n",
    "\n",
    "# Write the dictionary to JSON\n",
    "with open('features.json', 'w') as f:\n",
    "    json.dump(serializable_features_dict, f, indent=4)\n",
    "\n",
    "print(\"Data saved to features.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
